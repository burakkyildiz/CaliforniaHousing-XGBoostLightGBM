# ğŸ¡ California Housing Price Prediction â€” XGBoost vs LightGBM

> ğŸ“˜ **Description:**  
> Comparison of XGBoost and LightGBM model performance **before and after applying Box-Cox & Yeo-Johnson transformations**,  
> using the [California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices) dataset.

---

## ğŸ§¾ TL;DR

- **Goal:** Predict median house values based on demographic and geographical features  
- **Dataset:** [California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)  
- **Models:** XGBoost, LightGBM (default & tuned)  
- **Transformations:** Box-Cox & Yeo-Johnson  
- **Best RÂ²:** 0.8228 (LightGBM with hyperparameters, no transformation)

---

## ğŸ“Š Dataset Information

**Target variable:** `median_house_value`

**Features:**  
`['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity']`

**Dataset size:** ~20,000 samples  
**Source:** [Kaggle - California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)

---

## ğŸ“ˆ Feature Distributions

Below are the **feature distributions** of the California Housing dataset,  
before and after applying the **Box-Cox** and **Yeo-Johnson** transformations.

### ğŸŸ© Before Transformation
![Before Transformation](images/distributions_before_transformation.png)

### ğŸŸ¦ After Transformation
![After Transformation](images/distributions_after_transformation.png)

ğŸ“‰ *Observation:*  
Most features (like `total_rooms`, `population`, and `median_house_value`) were right-skewed.  
After applying transformations, their distributions became more symmetric and closer to normal.

---

## âš™ï¸ Model 1: Without Transformation

| Model | MAE | MSE | RMSE | RÂ² |
|-------|------|------|------|------|
| XGBoost (Default) | 28663.59 | 1.78Ã—10â¹ | 42221.55 | 0.8071 |
| XGBoost (Tuned) | 27896.12 | 1.69Ã—10â¹ | 41076.44 | 0.8175 |
| LightGBM (Default) | 29221.30 | 1.80Ã—10â¹ | 42480.51 | 0.8048 |
| LightGBM (Tuned) | **27291.08** | **1.64Ã—10â¹** | **40465.69** | **0.8228** |

ğŸ“ˆ *Best performer:* **LightGBM with hyperparameter tuning**  
ğŸ’¡ *Insight:* Hyperparameter tuning clearly improved both models, especially LightGBM.

---

## âš™ï¸ Model 2: After Box-Cox & Yeo-Johnson Transformation

| Model | MAE | MSE | RMSE | RÂ² |
|-------|------|------|------|------|
| XGBoost (Default) | 28214.20 | 1.81Ã—10â¹ | 42602.63 | 0.8036 |
| XGBoost (Tuned) | 27416.49 | 1.72Ã—10â¹ | 41437.39 | 0.8142 |
| LightGBM (Default) | 28943.76 | 1.85Ã—10â¹ | 43032.55 | 0.7997 |
| LightGBM (Tuned) | **27214.95** | **1.69Ã—10â¹** | **41072.79** | **0.8175** |

ğŸ“ˆ *Observation:*  
Data transformations (Box-Cox, Yeo-Johnson) **did not significantly improve** performance on tree-based models like XGBoost and LightGBM â€”  
these algorithms already handle non-normal feature distributions quite well.

---

## ğŸ§  Insights

- Tree-based models are robust to skewed feature distributions, so transformations like Box-Cox or Yeo-Johnson had **minor impact**.  
- **LightGBM** slightly outperformed **XGBoost** across all settings.  
- The strongest predictor was **`median_income`**, aligning with real-world housing trends.  
- Model tuning had a **larger effect** on performance than transformations.

---

## ğŸ“Š RÂ² Score Comparison

The following chart shows the **RÂ² performance** of all models before and after applying data transformations.

![R2 Comparison](images/model_r2_comparison.png)

ğŸ“ˆ *Insight:*  
LightGBM with hyperparameter tuning achieved the **best overall RÂ² (0.8228)**,  
while transformations like Box-Cox and Yeo-Johnson produced only minimal improvements.

---

## ğŸ§© Used Libraries

```python
pandas, numpy, seaborn, matplotlib  
scikit-learn, xgboost, lightgbm, scipy
